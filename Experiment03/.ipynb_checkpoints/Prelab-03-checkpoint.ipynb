{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58429c0-fe79-43b1-8c2d-694d392bd362",
   "metadata": {},
   "source": [
    "# Prelab 03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255fc3c3-2ff0-44c6-b75d-64b7cd4e4279",
   "metadata": {},
   "source": [
    "## Fitting damped harmonic oscillator transients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226e23fe-ea61-4536-add6-a8991e5418ba",
   "metadata": {},
   "source": [
    "The code below will generate data to fit. This will allow us to test the fitting code and make sure we understand it's limitations and how the data interacts with it. You can change the parameters as well as the spacing of points. Play around with the frequency and spacing and see how this changes your perception of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e020b50a-a17f-44c6-a79f-48fca649f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to calculate and plot the transient response of a damped, harmonic oscillator\n",
    "\n",
    "# import the  necessary libraries and rename them\n",
    "import numpy as np\n",
    "import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "# Define the Parameter Names, and give them numerical values\n",
    "\n",
    "param_names = [\"amplitude\", \"tau\", \"resonant-freq\", \"phase\"]\n",
    "input = (1, 0.002, 500, np.pi/2)\n",
    "\n",
    "# set the range and spacing of points in the generated data\n",
    "start=0.0\n",
    "end=0.02\n",
    "spacing=0.0001\n",
    "uncertainty=0.1 #set an uncertainty in V\n",
    "\n",
    "# Define the Function for the Harmonic Oscillator Transient\n",
    "\n",
    "def fit_function(x, amplitude, tau, resonantf, phase):\n",
    "    return amplitude * np.exp(-x/tau) * np.cos(2.0*np.pi * resonantf * x + phase)\n",
    "\n",
    "\n",
    "# Define a set of x values that will be used for the calculation\n",
    "# Note that in your fitting code, x is defined differently \n",
    "#  - do not change that part of the fitting code when you get there.\n",
    "\n",
    "x = np.arange(start, end, spacing)\n",
    "y_err = uncertainty * np.ones(len(x))\n",
    "noise = (uncertainty)*np.random.randn(len(x)) # generates normally distributed scatter \n",
    "\n",
    "y = fit_function(x, *input)+noise\n",
    "\n",
    "\n",
    "plt.errorbar(x,y,yerr=y_err,marker='.',linestyle=\"\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"Voltage\")\n",
    "plt.title(\"Damped Oscillator\")\n",
    "\n",
    "# save and plot image \n",
    "plt.savefig(\"DampedOscillator1.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fefc25-9786-4a65-b8e8-a80689a79485",
   "metadata": {},
   "source": [
    "Now try fitting the data you've just generated. Since you're not reading a file in this time, the code is already modified to directly read the arrays above, but you will need to set up the inputs to use the appropriate function. Try using the same \"guesses\" as you input as the parameters, as well as parameters that are very different to see how the routine responds. Start by generating a dense grid of data, where by eye you can clearly see the oscillations, then try sparse data. If your guesses are exact, the fit will likely still return similar values, but if you change the guesses the fit will find a new local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe4138",
   "metadata": {},
   "outputs": [],
   "source": [
    "gopts=VOLTAGE_VERSUS_TIME_GRAPH_OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066930f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RingdownFitParameters(\n",
    "    amplitude=1,\n",
    "    time_constant=0.002,\n",
    "    resonant_frequency=450,\n",
    "    phase=0\n",
    ")\n",
    "\n",
    "results = autofit_without_packing(x, y, y_err, gopts, model)\n",
    "results.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a10c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results.initial_guess_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a949a83d-5d58-4fd4-a9ea-b0917b1010df",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results.initial_guess_residuals_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results.autofit_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results.autofit_residuals_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea40ff05-a761-401d-8602-8564642f1644",
   "metadata": {},
   "source": [
    "### Advice to self for fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee1fdb-cbfc-4e03-b1d7-3d494d2e97d3",
   "metadata": {},
   "source": [
    "Describe what you found as you changed the density of points and guesses. Give yourself some advice for how to set yourself up for success when taking data, and setting up the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892222c5",
   "metadata": {},
   "source": [
    "- the more dense the data is, the better of a fit, but changes to the time constant need to be made\n",
    "- autogenerated data is almost always perfect for modeling with slight errors\n",
    "- higher uncertainty doesn't necessarily make a better fit, so it's important to properly estimate uncertainty!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141c3650-9239-43cd-a136-68ac6b81ffdd",
   "metadata": {},
   "source": [
    "## Analyzing noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a7379-99b9-450c-b5f7-31aba788b279",
   "metadata": {},
   "source": [
    "Let's take a bit closer look at how we are treating noise. The \"Data Pack and Trim\" routine we've given you uses the ($1\\sigma$) standard deviation of your selected region to determine the noise. If your noise is gaussian, which it often is if you have fluctuating values, then this is very reasonable. However, you have probably seen that for some of your data in Experiments 1 or 2 sometimes you just see bit noise where the values are mostly constant, but sometimes jump to another mostly constant value. This comes from the digitization of the data, and depending on the acquisition parameters can overwhelm any gaussian noise present in the system.\n",
    "\n",
    "This bit noise works like any other digital noise: the \"true\" value lies somewhere between with equal probablility since we simply have no way to tell. Formally this can be treated as a \"boxcar\" distribution of possible values and should be given an uncertainty of $\\frac{1}{2}\\delta_{resolution}/\\sqrt{3}$ which is the standard deviation of this boxcar distribution with a width given by the resolution.\n",
    "\n",
    "In Experiment 3 your data will have a fairly large dynamic range: the initial response to a kick will be large, and you'll want as much of the ringdown as you can use. With a large signal, and some filtering or averaging within the oscillsocope you are very likely to see digitization/bit noise this week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18275d94-2043-4041-a1f9-3a27dc180ba2",
   "metadata": {},
   "source": [
    "### Digital noise example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de299f-fab8-48ab-8090-baf2c423a376",
   "metadata": {},
   "source": [
    "Load the sample data file `LRC2CH1.csv` and isolate the first flat segment to examine the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0747f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, data = load_raw_data('LRC2CH1.csv', trim_range=(4995,5015), \n",
    "                     plot=True, graphing_options=gopts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be260da",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = calculate_uncertainty(raw_data, method=\"default\", \n",
    "                                indices_range=(0,5000),\n",
    "                                y_range=(0.437,0.443),\n",
    "                                plot=True, graphing_options=gopts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d4f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_data = pack_data(data, std_dev, p=3, \n",
    "                        plot=True, graphing_options=gopts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f005bc22-d85e-4853-84a2-f69dc914a6d6",
   "metadata": {},
   "source": [
    "Now, look at the data, and look at the standard deviation. Do you believe the uncertainty in this measurement corresponds to the standard deviation given here? Comment on the value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da1952d",
   "metadata": {},
   "source": [
    "The standard deviation doesn't seem to adequately represent the uncertainty, because the data is almost uniform. In this case, a digital uncertainty seems more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843a20ca-e971-4056-adb9-a409e02957e4",
   "metadata": {},
   "source": [
    "Another factor is the resolution of the oscilloscope on this particular vertical sensitivity. The data here rarely changes enough to even show any other values, but because it happens to here, we can determine the resolution from the bits jumped between. The Data Pack and Trim package has an option to analyze the \"digital\" noise (i.e. resolution limited) with the formula above. Try it now... change the `uncertainty` variable (on line 29) to the string `digital` and look at the output. Compare this to the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "digital_uncertainty = calculate_uncertainty(data, method=\"digital\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64340343",
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_data = pack_data(data, digital_uncertainty, p=3, \n",
    "                        plot=True, graphing_options=gopts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862c6e50-ce51-4be8-bc67-1fd72d11501c",
   "metadata": {},
   "source": [
    "Which uncertainty, the standard deviation or the digital resolution uncertainty seems more appropriate here? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f44ac",
   "metadata": {},
   "source": [
    "The digital resolution seems much better. The model would fit considerably better with these greater uncertainties, whereas with the previous estimate, no model would be able to sufficiently match that data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a58fb45-da86-42ad-a244-8b968ab0c6c3",
   "metadata": {},
   "source": [
    "## Creating arrays and adding to them (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d3e87-e44a-46a3-b954-72589d2c442a",
   "metadata": {},
   "source": [
    "In experiment 4, we'll use a limited spreadsheet-like interface (you may have seen it in 119), but if you want to store data that's being output by other parts of the code, you can create an array of that data on the fly. We'll use a numpy array because that will also allow us to do calculations and array operations.\n",
    "\n",
    "To create a numpy array we use the command: `np.array()`\n",
    "Only the first argument is required, and it needs to itself be an array.\n",
    "These are defined within square brackets with elements separated by commas: `[element1,element2,...]`\n",
    "A higher dimensional array is created by nesting the square brackets: `[[element11,element12],[element21,element22]]`\n",
    "and so on.\n",
    "\n",
    "Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9afa90-a99e-4441-9feb-cd38c25dbe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([1,2,3]) # a 1D array of dimension 1x3\n",
    "print(\"A =\",A)\n",
    "\n",
    "B = np.array([[1],[2],[3]]) # a 2D array of dimension 3x1\n",
    "print(\"B =\",B)\n",
    "\n",
    "C = np.array([[11,12,13],[21,22,23]]) # a 2x3 2D array (a matrix\n",
    "print(\"C =\",C)\n",
    "\n",
    "Z = np.zeros((4,2)) # a 4x2 2D array of zeros, the dimensions need to be specified in brackets except in 1D\n",
    "print(\"Z =\",Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33f25fa-28ff-4339-b6ba-4985469ff0c8",
   "metadata": {},
   "source": [
    "If you want the first (ahem, the zeroth) element, you can specify that element and pull it out as a separate variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018800ae-eb8a-49da-9ced-f424676d35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=A[0] # only need to specify the index\n",
    "print(\"a =\",a)\n",
    "\n",
    "b=B[0,0] # a vertical array needs to be treated like a 2D array, \n",
    "#and specify the index of the row, and column, otherwise it returns a vector of 1 element\n",
    "print(\"b =\",b)\n",
    "\n",
    "c=C[0,0]\n",
    "print(\"c =\",c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250b91e-ce37-4556-a8e1-9a0a600cd52d",
   "metadata": {},
   "source": [
    "We can also replace a particular value by redefining the value for that index, and if you start with an array of zeros of the correct size, this is a good way to store data (but you need to know the final size of the array you expect):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cae612-0d5e-4842-80e4-67dc9b7b3420",
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0]=0\n",
    "print(\"A =\",A)\n",
    "\n",
    "Z[0]=[1,0.1]\n",
    "Z[1]=[2,0.2]\n",
    "Z[2,1]=0.3\n",
    "Z[3,0]=4\n",
    "print(\"Z =\",Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f54be2f-12e8-4d68-8e91-db29161b7e72",
   "metadata": {},
   "source": [
    "Now, let's make this more interesting! What if we want to add values to an existing array? We can \"append\" them. To do this we can use the `np.append(array, values, axis)` function. This requires specifying the array we want to add to, the values to be added, and which axis to add them to, being careful that the dimensions match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8126327-1a62-4fa3-ac7a-bfb20321e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.append(A, [4], axis=0) # this will update the original array A with one more element\n",
    "print(\"A =\",A)\n",
    "\n",
    "C1 = np.append(C, [[31,32,33]], axis=0) # this will add another row to array C and store it as C1\n",
    "print(\"C1 =\",C1)\n",
    "\n",
    "C2 = np.append(C, [[14],[24]], axis=1) # this will add another column to array C and store it as C2\n",
    "print(\"C2 =\",C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd10f02-346e-4661-8ce4-3c09744bb782",
   "metadata": {},
   "source": [
    "Now, run the cell above again! You will see that the array `A` continues to expand with the same additional element because we just redefined the variable! This can be useful for keeping a running tally of parameters, but you do have to be careful not to keep appending every time you run a cell (say to do something else)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d226c3-53e0-4cc4-8692-cc430e3aa9a1",
   "metadata": {},
   "source": [
    "### Make some arrays and modify them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1f64e-f116-44e4-870e-5670d23efc12",
   "metadata": {},
   "source": [
    "Now you try... (note you will probably want to print out some intermediate steps so you can check that you're doing what you think you are doing!)\n",
    "* Create an array of zeros that is 2 x 5 and store it as a variable name\n",
    "* Modify the first row to have values `[3,3]` and the 2rd row to have `[2,1.5]`\n",
    "* Add `6.2` to all elements of the array\n",
    "* Mulitply all elements by `10`\n",
    "* Append another row containing values `[16,8.8]`\n",
    "* Print the value of the 2nd row, 2nd column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da796bf-f289-46b7-920b-2c63e02268b1",
   "metadata": {},
   "source": [
    "### Let's make an array of some of the parameters from our fitting..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074032-f0ec-4abe-83b7-fdf015398b2a",
   "metadata": {},
   "source": [
    "Run the code we used at the start a few more times with different point spacing and initial guesses. We'll store the point spacing, initial frequency, the output fit frequency, and the $\\chi^2$ as we go for each run. We'll start by pulling the last values you used above to set up the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b2d40-bfd9-4b9f-904b-bb2b9ad7dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "realfreq=np.array([input[2]]) # the frequency is the 2nd element of the input array\n",
    "ptspacing=np.array([spacing])\n",
    "fitfreq=np.array([fit_params[2]]) # the frequency is again the 2nd element\n",
    "fitfreq_err=np.array([fit_params_error[2]]) # and we'll store the uncertainty as well\n",
    "chisquared=np.array([chi2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28888b3-4f08-4dd9-9dbf-995afb06709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Real frequency:\",realfreq)\n",
    "print(\"Point spacing:\",ptspacing)\n",
    "print(\"Fit frequency:\",fitfreq)\n",
    "print(\"Chi squared:\",chisquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce276f7-1cab-43ba-8a16-0d406a90c3db",
   "metadata": {},
   "source": [
    "Now, change some parameters (point spacing, initial guesses) and we'll append the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251047a-86f5-417f-96dc-346f00ed1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to calculate and plot the transient response of a damped, harmonic oscillator\n",
    "\n",
    "# import the  necessary libraries and rename them\n",
    "import numpy as np\n",
    "import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "# Define the Parameter Names, and give them numerical values\n",
    "\n",
    "param_names = [\"amplitude\", \"tau\", \"resonant-freq\", \"phase\"]\n",
    "input = (1, 0.002, 500, np.pi/2)\n",
    "\n",
    "# set the range and spacing of points in the generated data\n",
    "start=0.0\n",
    "end=0.02\n",
    "spacing=0.0008\n",
    "\n",
    "uncertainty=0.01 #set an uncertainty in V\n",
    "\n",
    "# Define the Function for the Harmonic Oscillator Transient\n",
    "\n",
    "def fit_function(x, amplitude, tau, resonantf, phase):\n",
    "    return amplitude * np.exp(-x/tau) * np.cos(2.0*np.pi * resonantf * x + phase)\n",
    "\n",
    "\n",
    "# Define a set of x values that will be used for the calculation\n",
    "# Note that in your fitting code, x is defined differently \n",
    "#  - do not change that part of the fitting code when you get there.\n",
    "\n",
    "x = np.arange(start, end, spacing)\n",
    "y_err = uncertainty*np.ones(len(x))\n",
    "noise = (uncertainty)*np.random.randn(len(x)) # generates normally distributed scatter \n",
    "\n",
    "y = fit_function(x, *input)+noise\n",
    "\n",
    "\n",
    "plt.errorbar(x,y,yerr=y_err,marker='.',linestyle=\"\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"Voltage\")\n",
    "plt.title(\"Damped Oscillator\")\n",
    "\n",
    "# save and plot image \n",
    "plt.savefig(\"DampedOscillator1.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe9c99-022a-49ce-a88d-43531bed001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load python packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "###############################################################################\n",
    "# DEFINED FITTING FUNCTIONS\n",
    "###############################################################################\n",
    "\n",
    "def sine_func(x, amplitude, freq, phase):\n",
    "    return amplitude * np.sin(2.0 * np.pi * freq * x + phase)\n",
    "\n",
    "def offset_sine_func(x, amplitude, freq, phase, offset):\n",
    "    return (amplitude * np.sin(2.0 * np.pi * freq * x + phase)) + offset\n",
    "\n",
    "def exponential_func(x, amplitude, tau, voffset):\n",
    "    return amplitude * np.exp(x/(-1.0*tau)) + voffset\n",
    "\n",
    "def ringdown_function(x, amplitude, tau, resonantf, phase):\n",
    "    return amplitude * np.exp(-x/tau) * np.cos(2.0*np.pi * resonantf * x + phase)\n",
    "\n",
    "def linear_func(x, slope, intercept):\n",
    "    return slope * x + intercept\n",
    "\n",
    "###############################################################################\n",
    "# LIST OF ALL INPUTS\n",
    "###############################################################################\n",
    "\n",
    "# Name of the data file\n",
    "#fname = \"sintest2_pack.csv\"\n",
    "\n",
    "# Names and units of data columns from fname\n",
    "x_name = \"Time\"\n",
    "x_units = \"s\"\n",
    "y_name = \"Voltage\"\n",
    "y_units = \"V\"\n",
    "\n",
    "# Modify to change the fitting function, parameter names and to set initial parameter guesses\n",
    "fit_function = ringdown_function\n",
    "param_names = (\"amplitude\", \"tau\", \"resonantf\", \"phase\")\n",
    "guesses = (1, 0.002, 200, 0.0)\n",
    "\n",
    "# Flags for optional features\n",
    "show_covariance_matrix = False\n",
    "set_xy_boundaries = False\n",
    "lower_x = -0.01 # these values ignored if set_xy_boundaries = False\n",
    "upper_x = 0.01\n",
    "lower_y = -1\n",
    "upper_y = 1\n",
    "\n",
    "###############################################################################\n",
    "# LOAD DATA\n",
    "###############################################################################\n",
    "\n",
    "# load the file fname and skip the first 'skiprows' rows\n",
    "#data = np.loadtxt(fname, delimiter=\",\", comments=\"#\", usecols=(0, 1, 2, 3), skiprows=2)\n",
    "\n",
    "# Assign the data file columns to variables for later use\n",
    "#x = data[:, 0]\n",
    "#y = data[:, 2]\n",
    "#y_sigma = data[:, 3]\n",
    "y_sigma=y_err\n",
    "\n",
    "###############################################################################\n",
    "# INITIAL PLOT OF THE DATA\n",
    "###############################################################################\n",
    "\n",
    "# Define 500 points spanning the range of the x-data; for plotting smooth curves\n",
    "xtheory = np.linspace(min(x), max(x), 500)\n",
    "\n",
    "# Compare the guessed curve to the data for visual reference\n",
    "y_guess = fit_function(xtheory, *guesses)\n",
    "plt.errorbar(x, y, yerr=y_sigma, marker=\".\", linestyle=\"\", label=\"Measured data\")\n",
    "plt.plot(\n",
    "    xtheory,\n",
    "    y_guess,\n",
    "    marker=\"\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=1,\n",
    "    color=\"g\",\n",
    "    label=\"Initial parameter guesses\",\n",
    ")\n",
    "plt.xlabel(f\"{x_name} [{x_units}]\")\n",
    "plt.ylabel(f\"{y_name} [{y_units}]\")\n",
    "plt.title(r\"Comparison between the data and the intial parameter guesses\")\n",
    "plt.legend(loc=\"best\", numpoints=1)\n",
    "plt.show()\n",
    "\n",
    "# calculate the value of the model at each of the x-values of the data set\n",
    "y_fit = fit_function(x, *guesses)\n",
    "\n",
    "# Residuals are the difference between the data and theory\n",
    "residual = y - y_fit\n",
    "\n",
    "# Plot the residuals\n",
    "plt.errorbar(x, residual, yerr=y_sigma, marker=\".\", linestyle=\"\", label=\"residuals\")\n",
    "plt.xlabel(f\"{x_name} [{x_units}]\")\n",
    "plt.ylabel(f\"Residual y-y_fit [{y_units}]\")\n",
    "plt.title(\"Residuals using initial parameter guesses\")\n",
    "plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# PERFORM THE FIT AND PRINT RESULTS\n",
    "###############################################################################\n",
    "\n",
    "# Use curve_fit to perform the fit\n",
    "# fit_function: defined above to choose a specific fitting function \n",
    "# fit_params: holds the resulting fit parameters\n",
    "# fit_cov: the covariance matrix between all the parameters\n",
    "#          (used to extract fitting parameter uncertanties)\n",
    "# maxfev=10**5: maximum number of fitting procedure iterations before giving up\n",
    "# absolute_sigma=True: uncertainties are treated as absolute (not relative)\n",
    "fit_params, fit_cov = curve_fit(\n",
    "    fit_function, x, y, sigma=y_sigma, \n",
    "    p0=guesses,absolute_sigma=True, maxfev=10**5)\n",
    "\n",
    "# Define the function that calculates chi-squared\n",
    "def chi_square(fit_parameters, x, y, sigma):\n",
    "    dof = len(x) - len(fit_params)\n",
    "    return np.sum((y - fit_function(x, *fit_parameters)) ** 2 / sigma**2)/dof\n",
    "\n",
    "# Calculate and print reduced chi-squared\n",
    "chi2 = chi_square(fit_params, x, y, y_sigma)\n",
    "print(\"Chi-squared = \", chi2)\n",
    "\n",
    "# Calculate the uncertainties in the fit parameters\n",
    "fit_params_error = np.sqrt(np.diag(fit_cov))\n",
    "\n",
    "# Print the fit parameters with uncertianties\n",
    "print(\"\\nFit parameters:\")\n",
    "for i in range(len(fit_params)):\n",
    "    print(f\"   {param_names[i]} = {fit_params[i]:.3e} Â± {fit_params_error[i]:.3e}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# (Optional) Print the covariance between all variables\n",
    "if show_covariance_matrix:\n",
    "    print(\"Covariance between fit parameters:\")\n",
    "    for i, fit_covariance in enumerate(fit_cov):\n",
    "        for j in range(i+1,len(fit_covariance)):\n",
    "            print(f\"   {param_names[i]} and {param_names[j]}: {fit_cov[i,j]:.3e}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# residual is the difference between the data and model\n",
    "x_fitfunc = np.linspace(min(x), max(x), len(x))\n",
    "y_fitfunc = fit_function(x_fitfunc, *fit_params)\n",
    "y_fit = fit_function(x, *fit_params)\n",
    "residual = y-y_fit\n",
    "\n",
    "###############################################################################\n",
    "# PRODUCE A MULTIPANEL PLOT, WITH SCATTER PLOT, RESIDUALS AND RESIDUALS HISTOGRAM\n",
    "###############################################################################\n",
    "\n",
    "# The size of the canvas\n",
    "fig = plt.figure(figsize=(7,15))\n",
    "\n",
    "# The scatter plot\n",
    "ax1 = fig.add_subplot(311)\n",
    "ax1.errorbar(x,y,yerr=y_sigma,marker='.',linestyle='',label=\"Measured data\")\n",
    "ax1.plot(x_fitfunc, y_fitfunc, marker=\"\", linestyle=\"-\", linewidth=2,color=\"r\", label=\"Fit\")\n",
    "ax1.set_xlabel(f\"{x_name} [{x_units}]\")\n",
    "ax1.set_ylabel(f\"{y_name} [{y_units}]\")\n",
    "ax1.set_title('Best Fit of Function to Data')\n",
    "\n",
    "# (Optional) set the x and y boundaries of your plot\n",
    "if set_xy_boundaries:\n",
    "    plt.xlim(lower_x,upper_x)\n",
    "    plt.ylim(lower_y,upper_y)\n",
    "# Show the legend. loc='best' places it where the date are least obstructed\n",
    "ax1.legend(loc='best',numpoints=1)\n",
    "\n",
    "# The residuals plot\n",
    "ax2 = fig.add_subplot(312)\n",
    "ax2.errorbar(x, residual, yerr=y_sigma,marker='.', linestyle='', label=\"Residual (y-y_fit)\")\n",
    "ax2.hlines(0,np.min(x),np.max(x),lw=2,alpha=0.8)\n",
    "ax2.set_xlabel(f\"{x_name} [{x_units}]\")\n",
    "ax2.set_ylabel(f\"y-y_fit [{y_units}]\")\n",
    "ax2.set_title('Residuals for the Best Fit')\n",
    "ax2.legend(loc='best',numpoints=1)\n",
    "\n",
    "# Histogram of the residuals\n",
    "ax3 = fig.add_subplot(313)\n",
    "hist,bins = np.histogram(residual,bins=30)\n",
    "ax3.bar(bins[:-1],hist,width=bins[1]-bins[0])\n",
    "ax3.set_ylim(0,1.2*np.max(hist))\n",
    "ax3.set_xlabel(f\"y-y_fit [{y_units}]\")\n",
    "ax3.set_ylabel('Number of occurences')\n",
    "ax3.set_title('Histogram of the Residuals')\n",
    "\n",
    "# Save a copy of the figure as a png \n",
    "plt.savefig('FittingResults.png')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d526b2-18b4-4168-9ab5-3ff4e6538cd6",
   "metadata": {},
   "source": [
    "Now let's append these new values to the array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780e700-fa47-4fd7-b321-957a4a114cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "realfreq=np.append(realfreq,[input[2]],axis=0)\n",
    "ptspacing=np.append(ptspacing,[spacing],axis=0)\n",
    "fitfreq=np.append(fitfreq,[fit_params[2]],axis=0)\n",
    "fitfreq_err=np.append(fitfreq_err,[fit_params_error[2]],axis=0)\n",
    "chisquared=np.append(chisquared,[chi2],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7c0ba-2843-40f6-89db-9afa22909098",
   "metadata": {},
   "source": [
    "And print out our new array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43abc744-8bdc-4f1b-8f4c-29c9d3ba6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Real frequency:\",realfreq)\n",
    "print(\"Point spacing:\",ptspacing)\n",
    "print(\"Fit frequency:\",fitfreq)\n",
    "print(\"Fit frequency uncert:\",fitfreq_err)\n",
    "print(\"Chi squared:\",chisquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73349e68-ab66-4ba5-a59c-f0383bf9d192",
   "metadata": {},
   "source": [
    "This is far from elegant, and when appending you need to take care not to accidentally add additional entries and scramble your data, but it's an easy way to start keeping track of parameter output as you go. Alternatively you can set up a suitably sized array of zeros using `np.zeros` from the start and slot in your entries, though this requires knowing in advance how large an array to make."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Physics 219",
   "language": "python",
   "name": "physics_219_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
